\documentclass[main.tex]{subfiles}
\begin{document}


\section{Path integral} \label{sec:path-integral}

% \subsection{In nonrelativistic quantum mechanics}

% In order to compute the probability amplitude that a particle is first found in a (one dimensional) position \(x_1 \) at a time \(t_1 \) and then at a position \(x_2 \) at a time \(t_2 \) we compute an integral in the form 
% %
% \begin{align}
% \braket{x_2 , t_2 }{x_1 , t_1  } 
% = \int \mathcal{D}q \exp( i \int_q L (x, \dot{x}
% , t) \dd{t})
% \,,
% \end{align}
% %
% where \(L\) is the Lagrangian of the particle, \(q (t)\) is a possible path it can take from \(x_1 \) to \(x_2 \), and the integral extends over all of them: the integration measure can be understood as the limit
% %
% \begin{align}
% \mathcal{D}q \sim \lim_{N \to \infty } \prod_{i=1}^{N-1} \dd{x_i}
% \,,
% \end{align}
% %
% where we ``slice'' the time interval \(t_2 - t_1 \) into \(N \to \infty \) equal parts, \(\dd{x_i}\) being the integration elements for the \(i\)-th slice. 

The formalism of the path integral is about assigning a probability density to the space of possible field configurations. 
This space is infinite-dimensional: in order to treat it as a vector space we need to find some kind of basis, such as the values of the field on a grid of points. 

The way to formally treat these problems starts from the theory of linear functionals; for this first section we will follow \textcite[]{zaidiFunctionalMethods1983} quite closely.

\subsection{Linear Functionals}

% Following \cite[]{zaidiFunctionalMethods1983}.

We start from the space of square-integrable functions \(q(x)\), endowed with a product and an orthonormal basis \(\phi _n\).
We consider (multi-)linear \emph{functionals}, which are maps from the space of square-integrable functions (or from tuples of them) to \(\mathbb{R}\) or \(\mathbb{C}\). 
These can be represented as functions of infinitely many variables, countably so if we use the basis \(\phi _n\), uncountably so if we use the continuous basis \(x\).

A functional \(F[q]\) can be represented as a power series 
%
\begin{align}
F[q] = \sum _{n=0}^{\infty } \frac{1}{n!} \prod_{i=1}^{n} \int \dd{x_i} q(x_i) f(x_1, \dots, x_n)
\,.
\end{align}

Examples of this are the exponential series corresponding to the function \(f(x)\), mapping \(q(x)\) to \(e^{(f, q)}\) where the brackets denote the scalar product in the space, and the Gaussian series corresponding to the kernel \(K(x, y)\), mapping \(q(x)\) to \(e^{(q, K, q)}\), where 
%
\begin{align}
(q, K, q) = \int \dd{x} \dd{y} q(x) q(y) K(x, y)
\,.
\end{align}

\textbf{Functional derivatives} describe how the output of the functional changes as the argument goes from \(q(x)\) to \(q(x) + \eta (x)\), where \(\eta (x)\) is small. 
This will be a linear functional of \(\eta \) to first order, so we define the functional derivative with the expression 
%
\begin{align}
\eval{F[q+\eta ] - F[q]}_{\text{linear order}} = \int \eta (y) \fdv{F}{q (y)} \dd{y}
\,.
\end{align}

The analogy to finite-dimensional spaces is as follows: the functional derivative \(\fdv*{F}{q(y)}\) corresponds to the \emph{gradient} \(\nabla^{i} F\), while the integral in the previous expression corresponds to the \emph{directional derivative} \((\nabla^{i} F) \eta^{j} g_{ij}\).
The metric is present since the gradient is conventionally defined with a vector-like upper index; in our infinite-dimensional space the scalar product is given by the integral.

Practically speaking, the most convenient way to calculate a functional derivative is by taking \(\eta (x)\) to be such that it only differs from zero in a small region near \(y\), and let us define 
%
\begin{align}
\delta \omega = \int \eta (x) \dd{x}
\,.
\end{align}

Then, we define 
%
\begin{align}
\fdv{F}{q(y)} = \lim_{ \delta \omega \to 0} \frac{F[q + \eta ] - F[q]}{ \delta \omega }
\,.
\end{align}

In order for the limit to be computed easily, it is convenient for \(\eta (x)\) to be in the form \(\delta \omega \times \text{fixed function}\),
so that we are only changing the normalization as we shrink \(\delta \omega \).
A common choice is then 
%
\begin{align}
\eta (x) = \delta \omega  \delta (x-y)
\,.
\end{align}

If we apply this procedure to the identity functional \(q \to q\), we find 
%
\begin{align}
\fdv{q(x)}{q(y)} = \lim_{ \delta \omega  \to 0} \frac{q (x) + \delta \omega \delta (x-y) -q (x)}{ \delta \omega } = \delta (x-y)
\,.
\end{align}

The variable \(q\) is one-dimensional, if instead we wanted to consider a multi-dimensional coordinate system \(q_\alpha \) by the same reasoning we would find 
%
\begin{align}
\fdv{q_\alpha (x)}{q_\beta (y)} = \delta_{\alpha \beta } \delta (x-y)
\,.
\end{align}

An example: the functional derivative of a functional \(F_n\) defined by 
%
\begin{align}
F_n[q] = \int f(x_1 , \dots, x_n) q(x_1 )\dots q(x_n) \dd{x_1} \dots \dd{x_n}
\,,
\end{align}
%
where \(f\) is a symmetric function of its arguments, is given by 
%
\begin{align}
\fdv{F_n}{q(y)} = n \int f(x_1, \dots, x_{n-1}, y) q(x_1 )\dots q(x_{n-1}) \dd{x_1 } \dots \dd{x_{n-1}}
\,,
\end{align}
%
a function of \(y\). 

A \textbf{linear transformation} is in the form 
%
\begin{align}
q(x) = \int K(x, y) q'(y) \dd{y}
\,.
\end{align}

If this transformation has an inverse, which is characterized by the kernel \(K^{-1}\), then we must have the orthonormality relation 
%
\begin{align}
\int K(x,y) K^{-1} (y, z) \dd{y} =
\int K^{-1}(x,y) K (y, z) \dd{y} =
\delta (x-z)
\,.
\end{align}

We can do \textbf{Legendre transforms}: if we have a functional \(F\) we can differentiate with respect to the coordinate \(q\) to find 
%
\begin{align}
\fdv{F[q]}{q(x)} = p(x)
\,,
\end{align}
%
in analogy to the momentum in Lagrangian mechanics. Then, we can map \(F[q]\) to a new functional \(G[p]\) which will only depend on the momentum: 
%
\begin{align}
G[p]= F[p] - \int q(x) p(x) \dd{x}
\,.
\end{align}

We can also define functional integration, by 
%
\begin{align}
\int F[q] \qty[ \dd{q}] = \int \hat{F}(\qty{q_i}) \prod_i \dd{q_i}
\,.
\end{align}

On the right-hand side we are using the expression of the functional as a function of infinitely many variables which we discussed above;
we are then integrating over each of the coordinates in this infinite dimensional function space.
The infinite-dimensional measure is also often denoted as \(\mathcal{D}q\). 

This integral will not always exist, however in the cases in which it does we can change variables. 
Let us consider a linear change of variable, whose kernel is \(K(x, y)\), such that (compactly written) \(q = K q'\). 

Then, we want to compute the integral 
%
\begin{align}
\int F[Kq'] \qty[ \dd{Kq'}] 
\,
\end{align}
%
as an integral in \(\qty[ \dd{q}]\): in order to do so, we need to relate the two functional measures. 
We start by expressing both \(q\) and \(q'\) in terms of an orthonormal basis \(\phi _i\): inserting this into the linear transformation law we get 
%
\begin{align}
q(x) &= \int K(x, y) q'(y) \dd{y}  \\
\sum _{i} q_i \phi _i(x) &= \int K(x, y) \sum _{j} q'_j \phi _j (y) \dd{y}  \\
\sum _{i} q_i \underbrace{\int \phi _i (x) \phi _k (x) \dd{x}}_{ \delta_{ik}} 
&= 
\sum _{j} q'_j \underbrace{\int K(x, y) \phi _j (y) \phi _k (x) \dd{y} \dd{x}}_{ k_{jk}}  \\
q_k &= \sum _{j} q'_j k_{jk}
\,.
\end{align}

Then, the measure will transform with the determinant \(\det K = \det k\), which we can now express as an infinite product of the eigenvalues of \(k\): 
%
\begin{align}
\qty[ \dd{q}] = \abs{\pdv{q}{q'}} \qty[ \dd{q'}] = \det K \qty[ \dd{q'}]
\,.
\end{align}

Usually functional integrals cannot be computed analytically; the exception is given by \textbf{Gaussian integrals}, which generalize the finite-dimensional result 
%
\begin{align}
\int_{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + i b_j x_j) \dd{x_1 } \dots \dd{x_j} = \sqrt{\frac{(2 \pi)^{n}}{\det A}}
\exp(- \frac{1}{2} (A^{-1})_{ij} b_i b_j)
\,.
\end{align}

Here \(A_{ij}\) is an \(n\)-dimensional real matrix (which WLOG can be taken to be symmetric) while \(b_i\) is an \(n\)-dimensional vector.
The result comes from a transformation of the coordinates according to the finite-dimensional 

This can be interpreted as a ``functional'' (still finite-dimensional, so just a function, but we will generalize soon) of \(b_i\); we write it with an additional normalization \(N\) for convenience:
%
\begin{align}
Z[b] = N \int _{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + b_j x_j) \dd{x_1 } \dots \dd{x_j}
= N \sqrt{\frac{(2 \pi )^{n}}{\det A}} \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,,
\end{align}
%
and if we rescale the normalization \(N\) so that \(Z[\vec{0}] = 1 \) we get 
%
\begin{align}
Z[b] = \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,.
\end{align}

The infinite-dimensional generalization of this result amounts to replacing all the sums (expressed implicitly with Einstein notation here) with integrals; also conventionally we change the names of the variables to \(x \to q\), \(A \to K\), \(b \to J\): 
%
\begin{align} \label{eq:gaussian-integral}
Z[J] &= N \int \mathcal{D}q \exp(- \frac{1}{2} \int \dd{x} \dd{y} K(x, y) q(x) q(y) + i \int \dd{x} q(x) J(x))  \\
&= \exp(- \frac{1}{2} \int \dd{x} \dd{y} J(x) J(y) K^{-1}(x, y))
\,.
\end{align}

Let us now give some examples of applications of this result: \(K(x,y) = \sigma^{-2} \delta (x-y)\) means \(K^{-1}(x,y) = \sigma^2 \delta (x-y) \), so 
%
\begin{align}
Z[J] = \exp(- \frac{\sigma^2}{2} \int \dd{x} J^2(x))
\,.
\end{align}

This, as we shall see, can be used to give us a description of white noise, which is uncorrelated in momentum space.

\subsection{The probability density functional}

We can interpret the quantity 
%
\begin{align}
\exp(- \frac{1}{2} (q, K, q)) \mathcal{D}q
\,
\end{align}
%
as a Gaussian \emph{probability density functional} \(\dd{\mathcal{P}}[q]\), since 
\begin{enumerate}
    \item it is positive definite;
    \item it is normalized, as long as we set its integral, \(Z[0]\), to 1;
    \item it goes to zero as \(q \to \pm \infty \).
\end{enumerate}

If this is the case, then we ought to be able to compute the average value of a functional \(F[q]\) as 
%
\begin{align} \label{eq:average-of-a-functional}
\expval{F[q]} = \int F[q] \dd{P}[q] = \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) F[q]  
\,,
\end{align}
%
which we can generalize to any non-gaussian probability density functional by replacing the exponential \(\exp(- \frac{1}{2} (q, K, q))\) with a generic \(\mathcal{P}[q]\).

A useful kind of average we can compute is given by the \(N\)-point correlation function, 
%
\begin{align}
C^{(N)}(x_1, \dots, x_n) = \expval{q(x_1) \dots q(x_n)}
\,.
\end{align}

With the formula we gave earlier, this can be computed as 
%
\begin{align}
C^{(N)}(x_1, \dots , x_n)
= \int \mathcal{D}q \mathcal{P}[q] \prod_i q(x_i)
\,.
\end{align}

Here we can make use of a trick: consider the functional derivative
%
\begin{align}
\eval{\frac{1}{i}\fdv{Z[J]}{J(x_1 )}}_{J = 0} &= \frac{1}{i} \eval{\fdv{}{J(x)}}_{J=0} \int \mathcal{D}q
\exp(- \frac{1}{2} (q, K, q) + i (J,q))  \\
&= \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) q(x_1 )
= \expval{q(x_1 )} = C^{(1)}(x_1 )
\,,
\end{align}
%
which actually holds for any probability density functional, we did not make use of the gaussianity.
In general, each functional derivative ``brings down a factor \(q(x)\)'', so we will be able to write the \(N\)-point correlation function as
%
\begin{align}
C^{(N)} (x_1 \dots x_n) = \frac{1}{i^{N}} \eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 }
\,.
\end{align}

The correlation functions, which as we discussed in an earlier section are crucial when discussing structure formation, can be ``simply'' calculated by functional differentiation as long as we have the generating functional \(Z[J]\).
This generating functional is very similar mathematically to a partition function in statistical mechanics, and it serves an analogous role: its derivatives allow us to characterize the dynamics of the system. 

Now, any functional \(\mathscr{F} [q]\) can be expressed through a functional Taylor series:
%
\begin{align}
\mathscr{F}[q] = \sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} q(x_1) \dots q(x_n)
\,,
\end{align}
%
so if we compute the average value \(\expval{\mathscr{F}[q]}\) we find 
%
\begin{align}
\expval{\mathscr{F}[q]} &= 
\sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} \underbrace{\expval{q(x_1) \dots q(x_n)}}_{= C^{(N)} (x_1 \dots x_n)}  \\
&=\sum _{n=0}^{\infty } \frac{(-i)^{n}}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} 
\eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 } \\
&= \mathscr{F}\qty[ -i \fdv{}{J}]\eval{ Z[J]}_{J=0}
\,.
\end{align}

The expression with the functional \(\mathscr{F}\) being calculated ``at'' the functional derivative is purely formal: it allows us to compactly write the previous Taylor expansion, and should be interpreted as a shorthand for it.  

\subsection{Gaussian fields' correlation functions}

Consider a Gaussian field, whose partition function \(Z[J]\) is given, as we have shown in \eqref{eq:gaussian-integral}, by 
%
\begin{align}
Z[J] = \exp( - \frac{1}{2} (J, K^{-1}, J))
\,.
\end{align}

Then, as before we can calculate the correlation functions through functional derivatives: the first ones are 
%
\begin{align}
\expval{q(x)} &= \frac{1}{i} \eval{\fdv{Z[J]}{J(x)}}_{J=0}  \\
&= \eval{-i \int \dd{y} K^{-1}(x, y) J(y) \exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0} =0\\
\expval{q(y)q(x)} &= -\eval{\frac{ \delta^2 Z[J]}{ \delta  J(y) \delta  J (x)}}_{J=0} \\
&= -\eval{\qty(-K^{-1}(x, y) + \int \dd{z} \dd{w} K^{-1} (x, z)K^{-1} (y, w) J(w))\exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0}  \\
&= K^{-1} (x, y)
\,.
\end{align}

So, we have our result: \emph{for a Gaussian variable, the two-point correlation function is the inverse of the kernel}. 
This is perfectly analogous to the result we find for a zero-mean \(n\)-dimensional multivariate Gaussian with covariance matrix \(K^{-1}\): its probability density function is given by 
%
\begin{align}
\mathcal{N}(\vec{x} | \vec{0}, K^{-1}) = 
\frac{1}{(2 \pi )^{n/2} \sqrt{\det K^{-1}}}
\exp(- \frac{1}{2} \vec{x}^{\top} K \vec{x})
\,,
\end{align}
%
and its one and two point functions read \(\expval{x_i} = 0\), \(\expval{x_i x_j} = K^{-1}_{ij}\) respectively. 

Coming back to the infinite-dimensional scenario: a similar (albeit longer) calculation to the one before allows us to compute the \(N\)-point correlation function for the same Gaussian variable: we expand the exponential in \(Z[J]\) in a power series, and when we differentiate it an even number of times we find for the even-numbered correlation functions
%
\begin{align}
C^{2N}(x_1 \dots x_{2N}) &= \qty[K^{-1}(x_1, x_2 ) K^{-1} (x_3, x_4) \dots K^{-1} (x_{2N-1}, x_{2N})]_{\text{symmetrized}}
\,,
\end{align}
%
where ``symmetrized'' means that we must sum over all the permutations of the variables \(x_i\) in the argument of the inverse kernels; on the other hand, the odd correlation functions \(C^{2N+1}\) all vanish since they correspond to the integrals of odd functions over all space.

As we mentioned in section \ref{sec:statistical-methods}, in the Gaussian case the two-point function (or, equivalently, the kernel) contains all the information. 
Starting from it, we can reconstruct the Gaussian probability density functional \(\dd{\mathcal{P}}[q]\). 

% In the Gaussian case, as long as we know the two-point function, which corresponds to the inverse kernel, we can reconstruct the \(N\)-point function. 

\subsubsection{Connected correlation functions}

In the Gaussian case the \(N\)-point correlation functions are redundant for \(N > 2\). We wish to introduce a notion of ``irreducible'' correlation functions which minimize the amount of redundancy: formally, we require that in the Gaussian case the ones with \(N > 2\) vanish. 

% This can also be stated by saying that the ``irreducible'' \(N\)-point functions are all zero except for \(N=2\), since all the higher ones can be reduced to that one.

These irreducible functions are called the connected \(N\)-point correlation functions, and as we will check in a moment they can be calculated through the \emph{generating functional of connected correlation functions}:
%
\begin{align}
\mathscr{W}[J] = \log Z[J]
\,,
\end{align}
%
through the expansion 
%
\begin{align}
\mathscr{W}[J] &= \sum _{n=1}^{\infty } \frac{i^{n}}{n!} \int \dd{x_1} \dots \dd{x_n} C^{N}_{C} (x_1 \dots x_N) J(x_1) \dots J(x_n)  \\
C^{n}_C (x_1 \dots x_n) &= \frac{1}{i^{n}} 
\eval{\frac{ \delta^{n} \mathscr{W}[J]}{\delta  J(x_1) \dots \delta J(x_n) }}_{J = 0}
\,.
\end{align}

% These connected correlation functions \(C^{N}_{C}\) are not the same as the \(C^{N}\) from before, although they are related.

We could also have defined \(\mathscr{W}[J] = i \log Z[J]\), this is a matter of convention. 

In the Gaussian case we have 
%
\begin{align}
\mathscr{W}[J] = \log Z[J] = - \frac{1}{2} (J, K^{-1}, J)  
\,,
\end{align}
%
which, by direct comparison with the Taylor expansion, means that 
%
\begin{align}
C^{2}_{C} (x_1 , x_2 ) = K^{-1} (x_1 , x_2 )
\,,
\end{align}
%
while \(C^{N} \equiv 0\) for any \(N \neq 2\). 
This is a validation of our ansatz: in the Gaussian case it is the construction we hoped to make. 

\subsection{Computing probabilities}

We now wish to apply the construction of probability density functionals: what we will typically ask about, besides correlation functions, is the probability density that the field will attain certain values at certain points.
In order to describe this we will need to introduce some mathematical tools, making use of both Legendre and Fourier transforms. 
What we will achieve is a general framework, although its application to non-Gaussian cases is complicated, therefore we will only treat Gaussian examples. 

We define the \emph{classical field}
%
\begin{align}
q _{\text{cl}}(x) = \fdv{\mathscr{W}[J]}{J(x)}
\,,
\end{align}
%
and the effective action \(\Gamma [q _{\text{cl}}]\) as the Legendre transform of \(\mathscr{W}[J]\): 
%
\begin{align}
\Gamma [q _{\text{cl}}] = \mathscr{W}[J] - \int \dd{x} q _{\text{cl}} (x) J(x) 
\,,
\end{align}
%
from which we can then recover \(J(x)\) as 
%
\begin{align}
J(x) = - \fdv{\Gamma [q _{\text{cl}}]}{q _{\text{cl}}(x)}
\,.
\end{align}

Also, our expression for \(q _{\text{cl}}\) yields 
%
\begin{align}
q _{\text{cl}}(x) = -\int \dd{y} K^{-1}(x, y) J(y) 
\,,
\end{align}
%
from which we can express \(J(x)\) by using the direct kernel \(K(x, y)\): 
%
\begin{align}
\int \dd{x} q _{\text{cl}} (x) K (x, w) = - \int \dd{y} \dd{w} K^{-1}(x, y) K(x, w) J(y) = - \int \dd{w} \delta (y- w) J(y) = - J(w)
\,.
\end{align}

With an analogous procedure we can show that
%
\begin{align}
(J, K^{-1}, J) = (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

% \todo[inline]{So in some sense \(J\) is a covariant vector while \(q\) is a contravariant one, right? can this be said in a better way?}

The effective action then reads
%
\begin{align}
\Gamma [q _{\text{cl}}] &= \eval{- \frac{1}{2} (J, K^{-1}, J) + (J, K^{-1}, J)}_{J = J(q _{\text{cl}})} \\
&= \frac{1}{2} (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

Now we have the tools to consider actual probabilities: starting from our classical field \(q\), we want to compute the probability that it takes on a certain value \(q \in (\alpha , \alpha + \dd{\alpha })\) at a point \(\overline{x}\): this is expressed with a probability density function in the form 
%
\begin{align}
\dv{P_q}{\alpha } =P_{q(x)} (\alpha; \overline{x})
\,.
\end{align}

We want to write this ``\(P(\alpha ) \dd{\alpha }\)'' in terms of the functional integral; in order to do so, we start from the Fourier transform 
%
\begin{align}
\int \dd{\beta } \exp(i \beta \varphi ) P_q (\beta ; \overline{x}) = \expval{e^{i \beta \varphi }}_{\beta }
\,.
\end{align}

The integral is a definite one, with bounds corresponding to the region in which \(P_q\) is nonzero, typically \(\mathbb{R}\).
The right-hand side is an average over the possible values taken on by the field \(q\) at the point \(\overline{x}\), but it can also be computed by averaging over the possible \emph{overall} field configurations, computed at a point \(\overline{x}\):
% This also holds if we average over the position \(x\): 
%
\begin{align}
\int \dd{\beta } \exp(i \beta \varphi ) P_q (\beta ; \overline{x}) = \expval{e^{i q(\overline{x}) \varphi }}_{q }
\,.
\end{align}

Although the expression is similar we have made a large conceptual leap: \(\expval{}_\beta \) denotes a one-dimensional integral, the Fourier transform, while \(\expval{}_q\) denotes an \emph{infinite}-dimensional functional integral, for which we must employ calculation tools such as \eqref{eq:average-of-a-functional}. 
% This will be much harder to compute: it is a proper functional integral, to be computed according to \eqref{eq:average-of-a-functional}; however it must give the same result.
The need for this step is motivated by the fact that the values of the field at different points are not independent: what we want to study are precisely the correlations between them, so in order to treat a single point we must consider the whole field.

% \todo[inline]{In the notes this is denoted as ``averaging over \(q\)'', but I do not see how that makes sense. Is \(\overline{x}\) a typical, generic position?}

If we take the Fourier antitrasform, we find 
%
\begin{align}
P_q (\alpha , \overline{x}) &= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \varphi \alpha } \expval{e^{i \varphi q (\overline{x})}}_q  \\
&= \frac{1}{2 \pi } \int \dd{\varphi } \expval{e^{i \varphi (q(\overline{x}) - \alpha )}}_q  \\
&= \expval{ \delta (q(\overline{x}) - \alpha ) }_q
\,.
\end{align}

This equation can be interpreted to mean the following: ``the probability that the field \(q\) is equal to \(\alpha \) at \(\overline{x}\) is given by the integral of the probabilities of all the field configurations which satisfy \(q(\overline{x}) = \alpha \)''.

This formalism can be generalized to \(N\)-point functions, the notation for the probability that for \(j = 1 \dots N\) the field \(q\) takes on the value \(\alpha _j\) at position \(x_j\) is as follows: 
%
\begin{align}
\dd{P}_q^{N} &= P_q \qty(\alpha_1 \dots \alpha _N; x_1 \dots x_N) \dd{\alpha_1 } \dots \dd{\alpha _N}  \\
&= P_q \qty([\alpha _N]; [x_N]) \dd{\alpha_1 } \dots \dd{\alpha _N}  \\
P_q \qty([\alpha _N]; [x_N]) 
&= \expval{\prod_{j=1}^{N} \delta (q(x_j) - \alpha _j)}_q
\,.
\end{align}

Statistical independence corresponds to the statement that the product can be brought out of the average, and we are interested in the general case, in which this does not happen.

The question we generally ask is: what is this probability? We can try to find an expression for it in terms of the partition function \(Z[J]\): we use once again the fact that 
%
\begin{align}
\delta (x) = \frac{1}{2 \pi } \int \dd{\varphi } e^{i \varphi x}
\,
\end{align}
%
to see that 
%
\begin{align}
P_q ([\alpha _N]; [X_N]) = \frac{1}{(2 \pi )^{N}} \int \dd{\varphi_1 } \dots \dd{\varphi _N} \exp(- i \sum _{j=1}^{N} \varphi _j \alpha _j)
\expval{\exp(i \sum _{j=1}^{N}\varphi _j q(x_j))}_q
\,.
\end{align}

This can be brought back to the partition function by making use of the fact that 
%
\begin{align}
Z[J] &= \expval{\exp(i (J, q))}_q  \\
&= \int \mathcal{D}q P[q] \exp( i \int \dd{x} J(x) q(x))
\,,
\end{align}
%
therefore 
%
\begin{align}
\expval{\exp(i \sum _{j=1}^{N}\varphi _j q(x_j))}_q
= Z \qty[ \sum _{j=1}^{N} \varphi _j \delta (x - x_j)] 
= Z[ \widetilde{J}_\varphi ]
\,,
\end{align}
%
where we have used the fact that 
%
\begin{align}
i \int \dd{x} q(x) \qty(\sum _{j=1}^{N} \varphi _j \delta (x-x_j))
= i \sum _{j=1}^{N} \varphi_j q(x_j) 
\,.
\end{align}

So, we can compute the probability that the field reaches the values \(\alpha _j\) at the points \(x_j\) as long as we can compute the partition function at \(Z[\widetilde{J}_\varphi ]\), and then integrate \(N\) times: 
%
\begin{align}
P_q ([\alpha _N], [x_N]) = \frac{1}{(2 \pi)^{N}} 
\int \dd{\varphi_1 } \dots \dd{\varphi _N }
\exp(-i \sum _{j=1}^{N} \varphi _j \alpha_j )
Z[\widetilde{J}_\varphi ]
\,.
\end{align}

Let us compute this for the case of a Gaussian random field \(q(x)\). The partition function evaluated at \(\widetilde{J}_\varphi = \sum _{j} \varphi _j \delta (x- x_j)\) is equal to 
%
\begin{align}
Z[\widetilde{J}_\varphi ] &= \exp( - \frac{1}{2} (\widetilde{J}_\varphi , K^{-1}, \widetilde{J}_\varphi )) \\
&= \exp( - \frac{1}{2} \sum _{i, j =1}^{N} \varphi _i \varphi _j K^{-1} (x_i, x_j))
\,.
\end{align}

Since it appears in the expression for the partition function, let us define the \emph{covariance matrix}
%
\begin{align}
M_{ij} = K^{-1}(x_i, x_j) = C^{(2)} (x_i, x_j)
\,,
\end{align}
%
so that the probability reads, using the usual result about Gaussian integrals,
%
\begin{align}
P_q ([\alpha _N], [x_N]) &= \frac{1}{(2 \pi)^{N}} 
\int \dd{\varphi_1 } \dots \dd{\varphi _N }
\exp(-i \sum _{j=1}^{N} \varphi _j \alpha_j )
\exp(- \frac{1}{2} \varphi_i M_{ij } \varphi _j)  \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \det M}}
\exp(- \frac{1}{2} \alpha _i (M^{-1})_{ij} \alpha _j)
\,.
\end{align}

This is the standard expression for an \(N\)-variate Gaussian distribution whose covariance matrix is \(M_{ij}\). 

% \textbf{Application to Brownian motion}. 

\subsection{Avoiding divergences: high- and low-pass filters}

We will now discuss the application of the filter functions introduced in section \ref{sec:statistical-methods} to the path integral formulation. 
If \(W_R (x)\) is our filter function, then the general correlation function can be calculated by substituting the value of the field at a point with its convolution with \(W_R\):
%
\begin{align}
C^{(N)}_R (x_1 \dots x_N) &= \int \mathcal{D}q \mathcal{P}[q] \prod_{r = 1}^{N}
\int \dd{y_r} q(y_r) W_R (\abs{y_r - x_r})  \\
&= \int \prod_{r=1}^{N} \dd{y_r} W_R(\abs{y_r - x_r}) C^{(N)} (y_1 \dots y_N)
\,.
\end{align}

The same procedure can be applied to the \emph{connected} correlation functions, so we can calculate the \(C^{(N)}_{R, C}\): \emph{smooth, connected} \(N\)-point correlation functions. 

We can calculate these ``smoothed'' correlation functions through the generating functional \(Z[J]\) by choosing \(J(x)\) in the form 
%
\begin{align}
J(x) =\int \dd{y} \varphi (x+y) W_R (y)
\,,
\end{align}
%
where \(\varphi \) is a generic function.

Since these regularized correlation functions do not diverge at vanishing distances, we can define the \(N\)-th order \textbf{moment}:
%
\begin{align}
\expval{q_R^{N}} = C^{(N)}_R (x \dots x) 
\,
\end{align}
%
and the \(N\)-th order \textbf{cumulant}: 
%
\begin{align}
\expval{q_R^{(N)}}_C = C^{(N)}_{R, C} (x \dots x)
\,.
\end{align}

Due to homogeneity and isotropy, neither of these depends on \(x\). 
We expect these to diverge if we perform no filtering, which is equivalent to taking the filtering scale \(R \to 0\).
On the opposite limit, taking \(R \to \infty \) amounts to averaging over all space, so we expect \(\expval{q_R^{N}} \sim \expval{q}^{N}\), while \(\expval{q_R^{N}}_C \sim 0\). 

The moments of \(q_R\) can be obtained through the moment generating function 
%
\begin{align}
Z(\varphi ) = Z[J_\varphi (x) ] = Z[\varphi W_R (\abs{x})]
\,,
\end{align}
%
and we can define the \emph{cumulant} generating function \(W(\varphi )\) analogously. 
We can then define an effective action through a Legendre transform:
%
\begin{align}
\Gamma (q_{R, \text{cl}}) = W(\varphi ) - q_{R, \text{cl}} \varphi 
\,,
\end{align}
%
where the classical field \(q_{R, \text{cl}}\) is given by 
%
\begin{align}
q_{R, \text{cl}} = \dv{W(\varphi )}{\varphi }
\,.
\end{align}

The function \(Z(\varphi )\) is just the Fourier transform of \(P_R(\alpha , \overline{x})\): 
%
\begin{align}
Z(\varphi ) = \expval{e^{i \varphi q_R}}_q 
= \int  \dd{\alpha } e^{i \varphi \alpha } P_{q_R} (\alpha ; \overline{x}) 
\,,
\end{align}
%
so we can insert the explicit expression for the probability inside the inverse of this transform: 
%
\begin{align}
P_R(\alpha ; \overline{x}) &= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \alpha \varphi } Z(\varphi ) \\
&= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \alpha \varphi }
\int \mathcal{D}q \mathcal{P}[q] \exp(i \varphi \int \dd{y} W_R (\abs{y - \overline{x}}) q(y) )
\,.
\end{align}

We can expand \(Z(\varphi )\) and \(W(\varphi )\) as 
%
\begin{align}
Z(\varphi ) &= 1 + \sum _{N=1}^{\infty } \frac{i^{N}}{N!} \varphi^{N} \expval{q_R^{N}} \\ 
W(\varphi ) &=  \sum _{N=1}^{\infty } \frac{i^{N}}{N!} \varphi^{N} \expval{q_R^{N}}_C  
\,,
\end{align}
%
therefore 
%
\begin{align}
\expval{q_R^{N}} = \int \dd{\alpha } \alpha^{N} P_{q_R} (\alpha ; \overline{x})
\,,
\end{align}
%
which clarifies what was meant by saying that these are \emph{moments}. 
\(P_{q_R}\) can be reconstructed starting from these moments. 


We can take a different path, by transforming the probability measure: we start from the probability density functional evaluated at \(q\), and calculate the expectation value of a generic functional \(F[q]\) as 
%
\begin{align}
\int \mathcal{D}q P[q] F[q]
\propto \int \mathcal{D}q_R P[(W^{-1}, q_R)] F[(W^{-1}, q_R)]
\,.
\end{align}

The proportionality is because we need to include a Jacobian determinant, however probability densities must always be normalized to 1 so any constant is inessential. 

Applying this to a Gaussian field amounts to mapping the kernel \(K\) to a ``smoothed'' kernel \(K_R\), which corresponds to the inverse of the smoothed two-point correlation function: \(\expval{q_R(x) q_R(y)}\). 

% Now, we move to a more physical example. We consider a Gaussian field \(q\) in three dimensions, whose mean is zero and whose two-point function in Fourier space is 
% %
% \begin{align}
% \expval{q(k) q(k')} = (2 \pi )^3 P(\abs{k}) \delta^{(3)} (k - k')
% \,,
% \end{align}
% %
% where \(P(\abs{k})\) is called the \textbf{power spectral density} of the field \(q\): it quantifies how much of the power of the field is transmitted at each frequency. 

As we mentioned in section \ref{sec:statistical-methods}, the smoothed two-point function is given by 
% 
\begin{align}
\expval{q_R (x) q_R(y)} &= G_R ( \abs{x-y}) = \frac{1}{(2 \pi )^3}
\int \dd[3]{k} P(k) \widetilde{W}^2_R(k) e^{i k \cdot (x-y)}  \\
&= \frac{1}{2 \pi^2} \int_0^{\infty } \dd{k } k^2 P(k) \widetilde{W}^2_R (k) j_0 (k \abs{x-y})
\,,
\end{align}
%
where \(j_0 \) is a Bessel function of the first kind. 
Recall that the two point function is the inverse of the kernel: therefore, from the first expression we can read off the Fourier transform of \(K^{-1}_R\), or, inverting, 
%
\begin{align}
\widetilde{K}_R (k) = \frac{1}{P(k) \widetilde{W}_R^2(k)} 
\,.
\end{align}

Now, let us consider a typical power-spectrum: \(P(k) = A k^{n}\). 
It can be shown that the smoothed two-point function will be asymptotically (in the \(r \ll R\) or \(r \gg R\) limits) proportional to
%
\begin{align}
G_R (r) \propto \qty[\max(R, r)]^{- (n+3)}
\,.
\end{align}

% The \(n = 0\) case is \textbf{white noise}, for which \(G_R(0) \sim R^{-3}\). 

% \todo[inline]{Connection with a Poisson process\dots not clear}

% The \(n = -3\) case is \textbf{flicker noise} corresponds to \(1/f\) noise in 1D.

In the \(n > 0\) case the smoothed two-point function \(G_R(r)\) must be equal to zero somewhere, since we have the constraint\footnote{This comes from the following line of reasoning: since the field has zero mean, 
%
\begin{align}
\int \dd[3]{y} \expval{q_R (x) q_R (y)} = \expval{q_R (x) \int \dd[3]{y} q_R(y)} = 0
\,.
\end{align}}
%
\begin{align}
\int_0^{\infty } \dd{r} r^2 G_R(r) = 0
\,.
\end{align}

% \todo[inline]{What does this have to do with \(n>0\)? Isn't this constraint always present?}

The first zero-crossing of the correlation function, \(r_0 \) such that \(G_R(r_0 ) = 0\), is called the \textbf{coherence length}. 

% If \(n < 0\), we have fractal behaviour: as long as \(G_R(r) \gg 1\), the fractal dimension is \(D_F = -n\). 
% \todo[inline]{Clarify this\dots how would we show it?}

\subsection{Saddle-point expansion}

We want to show that in general, our probability measure can be written as 
%
\begin{align}
\mathcal{P}[q] = \exp(- \frac{1}{2} (q, K, q) - V[q])
\,,
\end{align}
%
where the potential term encompasses all the non-quadratic parts of the probability.
Linear terms can be removed with a change of variable, so we can expand it starting from third order: 
%
\begin{align}
V[q] = \sum _{n=3}^{\infty } \dd{x_1 } \dots \dd{x_n} K^{(n)} (x_1 \dots x_n) q(x_1 )\dots q(x_n)
\,.
\end{align}

% Suppose that we did not know this, and we wanted to write a generic functional in the form
% %
% \begin{align}
% Z[J] = \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q) - V[q] + i (q, J) )
% \,,
% \end{align}
% %
% with arbitrary \(V[q]\). 

We start by ntroducing the ``integrating by parts relation'':
%
\begin{align}
Z[J] &= \exp(- V \qty(-i \fdv{}{J})) 
\int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q) + i (q, J) )  \\
&= \exp(- V \qty(-i \fdv{}{J}))
\underbrace{\exp(- \frac{1}{2} (J, K^{-1}, J))}_{Z_0 [J]}  \\
&= \sum _{n=0}^{\infty } \frac{(-)^{n}}{n!} \qty[V \qty(-i \fdv{}{J})]^{n} Z_0 [J]
\,.
\end{align}

With this approach, we can recover the probability of the field attaining a certain value as a function of the cumulants \(\expval{q_R^{N}}_C\), since as we have shown earlier the function \(W(\varphi )\) can be expressed in terms of them.\footnote{The calculation needed to do so is quite involved, more details can be found in the second section of \textcite[]{matarresePathintegralApproachLargescale1986}. Here we merely meant to show that such a procedure is possible, by introducing all the tools needed to perform it.} 

The \textbf{saddle point expansion} is the following: we want to compute an integral in the form 
%
\begin{align}
J = \int \dd{\tau } \exp(
    - f(\tau )
)
\,,
\end{align}
%
so we choose a \(\tau_0 \) such that \(f' (\tau_0 ) = 0\), and then approximate \(f\) up to second order: 
%
\begin{align}
J &\approx \int \dd{\tau } \exp(
    - f(\tau_0 ) - \frac{1}{2} f''(\tau_0 ) (\tau - \tau_0  )^2
)  \\
&\approx \sqrt{\frac{2 \pi }{f''(\tau_0 )}} \exp(- f(\tau_0 ))
\,.
\end{align}

We are effectively approximating the integrand as a Gaussian, whose mean and variance are computed perturbatively at its stationary point. 

We can apply this to the calculation of path integrals: if the probability \(P(q)\) can be written in terms of \(Z(\varphi ) = \exp(W(\varphi ))\), then we have 
%
\begin{align}
P(q) &= \frac{1}{2 \pi } \int \dd{\varphi } \exp(-i \varphi q + W(\varphi ))
\,.
\end{align}

The classical variable \(q _{R, \text{cl}} = \overline{q}\) is defined as \(\overline{q}= W' (\varphi ) \), and the effective action is written in terms of it: \(\Gamma (\overline{q}) = W(\varphi ) - \overline{q} \varphi \). Therefore, 
%
\begin{align}
\dv{\overline{q}}{\varphi } = W''(\varphi )
\,,
\end{align}
%
while the derivative of the classical action reads 
%
\begin{align}
\Gamma '' (\overline{q})
= \dv{}{\overline{q}}
\qty(- \varphi ) 
= - \qty[W''(\overline{q})]^{-1}
\,.
\end{align}

This means that \(\dd{\varphi } = - \Gamma'' (\overline{q}) \dd{\overline{q}}\), which allows us to change variable: 
%
\begin{align}
P(q) = -\frac{1}{2 \pi } \int \Gamma '' (\overline{q}) \dd{\overline{q}}
\exp( i q \Gamma '(\overline{q}) + \Gamma (\overline{q}) - \overline{q} \Gamma ' (\overline{q}))
\,,
\end{align}
%
where we used the fact that 
%
\begin{align}
- i \varphi q + W(\varphi ) &= - i \varphi q + \Gamma (\overline{q}) + \overline{q} \varphi    \\
&= \Gamma (\overline{q}) + (\overline{q} - iq) \dv{\Gamma (\overline{q})}{\overline{q}}
\,.
\end{align}

For large \(q\), this oscillating integral is dominated by the points at which the phase does not change much: these are the stationary points of the argument of the exponential, 
%
\begin{align}
iq \Gamma'' (\overline{q})  
+ \Gamma '(\overline{q}) - \Gamma '(\overline{q}) - \overline{q} \Gamma '' (\overline{q}) 
=
\Gamma'' (\overline{q}) (iq - \overline{q} )
&= 0
\,,
\end{align}
%
meaning that \(\overline{q} = iq\).

We can then apply the saddle-point approximation: 
%
\begin{align}
P(q) &\approx \frac{e^{i \Gamma (iq)}}{2 \pi } \Gamma '' (iq)
\int \dd{\overline{q}} \exp(- \Gamma ''(iq) \frac{\qty(\overline{q} - iq)^2}{2})  \\
&\approx \sqrt{\frac{\Gamma '' (iq)}{2 \pi }} \exp(\Gamma (iq))
\,.
\end{align}

In the Gaussian case the probability reads 
%
\begin{align}
P(q) = \frac{1}{\sqrt{2 \pi \sigma _R^2}} \exp(- \frac{q^2}{2 \sigma _R^2})
\,,
\end{align}
%
so we can recognize \(\Gamma (\overline{q}) = \overline{q}^2 / (2 \sigma _R^2)\), while \(W (\varphi ) = -\varphi^2 / (2 \sigma _R^2)\). 

We can apply this approximation in order to prove the claim from the start of this section: suppose we had a generating functional in the form
%
\begin{align}
Z[J] = \int \mathcal{D}q \underbrace{\exp(- \frac{1}{2} (q, K, q) - V[q] + i (q, J))}_{\exp(\mathscr{F}[q, J])}
\,
\end{align}
%
with arbitrary \(V\).

% The functional \(\mathscr{F}[q, J]\) is the solution of the 
The functional integral will be dominated by the stationary points of \(\mathscr{F}[q, J]\): we define \(q_0 \) as the field configuration satisfying
%
\begin{align} \label{eq:functional-equation-for-q0}
\eval{\fdv{\mathscr{F}[q, J]}{q}}_{q_0 } = (K, q_0 ) + \eval{\fdv{V[q]}{q}}_{q_0 } - iJ = 0
\,.
\end{align}

This will depend on \(J\) in general. We then expand up to second order, as usual: 
%
\begin{align}
\mathscr{F}[q, J] \approx \mathscr{F}[q_0 , J] + \frac{1}{2} \int \dd{x} \dd{y} \eval{\frac{ \delta^2 \mathscr{F}[q, J]}{ \delta q(x) \delta q(y) }}_{q_0 } \eval{(q(x) - q_0 (x)) (q(y) - q_0 (y))}_{\text{symm}}
\,.
\end{align}

Thanks to the functional equation for \(q_0 \) \eqref{eq:functional-equation-for-q0}, we can write \(\mathscr{F}[q_0 , J]\) as 
%
\begin{align}
\mathscr{F}[q_0 , J] &= \frac{1}{2} (q_0 , K, q_0 ) + V[q_0 ] - i (J, q_0 )  \\
&= (K, q_0 ) + \eval{\fdv{V[q]}{q}}_{q_0 } - \frac{i}{2}  (q_0, J) 
\,.
\end{align}
%

This is usually the ``bottleneck'' in the calculation, practically speaking. It is really hard to solve this functional equation.
Also, we define
%
\begin{align}
\eval{\frac{ \delta^2 \mathscr{F}}{ \delta q(x_1 ) \delta  q(x_2 )}}_{q_0 }
= K(x_1 , x_2 ) + \eval{\frac{ \delta^2 V[q]}{ \delta q(x_1 ) \delta q(x_2 )}}_{q_0 } = \mathscr{Q}(x_1 , x_2 )
\,.
\end{align}

% \todo[inline]{This confuses me: earlier we said that \(V\) starts from third order, since the quadratic term is brought out. What are we doing here? If we only consider the Gaussian part of \(V\) what is even the point of including it?}

We then perform the integration with respect to the field \(q - q_0 \) (this amounts to adding a constant, so the measure does not change) and find 
%
\begin{align}
Z[q_0 , J] \approx \mathscr{N}\frac{\exp(- \mathscr{F}[q_0, J ])}{\sqrt{\det \mathscr{Q} [q_0, J]}}
\,.
\end{align}

Using the identity \(\log \det M = \Tr \log M\) we can compute the generating functional for cumulants:
%
\begin{align}
W[q_0 , J] \approx \mathscr{F}[q_0 , J] - \frac{1}{2} \Tr \log \mathscr{Q}[q_0, J] + \const
\,.
\end{align}

\subsection{Path integrals in Quantum Field Theory}

Let us consider another example, whose physical application is to describe the motion of a massive scalar boson with Lagrangian 
%
\begin{align}
\mathscr{L} = \underbrace{\frac{1}{2} \partial_{\mu } \phi \partial^{\mu } \phi - \frac{1}{2} \mu^2 \phi^2}_{\mathscr{L}_{0}}+ \mathscr{L}_I (\phi )
\,,
\end{align}
%
where the self-interaction term is some non-quadratic function of \(\phi \), often taken to be proportional to  \(\phi^3\)  or \(\phi^{4}\). 

The Feynman path integral corresponding to this Lagrangian is given by the functional 
%
\begin{align}
Z[J] = N \int \mathcal{D} \phi \exp(i \int \mathscr{L}(\phi ) +  J \phi \dd{x} )
\,.
\end{align}

Let us start with the non-interacting case, that is, we compute \(Z_0 \) with only the quadratic term in the Lagrangian. This can be expressed, in the formalism from before, using the kernel 
%
\begin{align}
K(x, y )= (- \square_x - \mu^2 ) \delta (x-y)
\,.
\end{align}

Now, the expression the functional is given in terms of \(K^{-1}\): what is the inverse of this kernel? The definition reduces to 
%
\begin{align}
\int K(x, y) K^{-1}(y, z) \dd{y} &= \delta (x-z)   \\
- \qty(\square_x + \mu^2) K^{-1} (x, z) &= \delta (x-z)
\,,
\end{align}
%
which is readily solved in momentum space, with a \(+i \epsilon \) prescription in order to avoid the pole in the integration: what we find is called the \emph{Green's function}, 
%
\begin{align}
K^{-1}(x, z) = G(x-z) = \frac{1}{(2 \pi )^{4}} \int \frac{e^{-ik \cdot (x-z)}}{k^2 + \mu^2 - i \epsilon } \dd{k} 
\,,
\end{align}
%
so the unperturbed functional reads 
%
\begin{align}
Z_0 [J] = \exp(- \frac{i}{2} \int \dd{x} \dd{y} G(x-y) J(x) J(y))
\,.
\end{align}

% This, evaluated at \(J =0\), can be used to calculate the \emph{propagator}, which quantifies the probability amplitude that a particle localized at a certain position at a certain time will be found at another position, at another time.
% This can be used to compute expectation 

This by itself might not seem very useful, the motion of a free massive boson can be calculated with easier methods.
However, the real power of this path integral is the possibility to write the interacting term perturbatively: the interaction Lagrangian is a function of \(\phi \), which is what we find if we perform a functional integration of the argument of the exponential in \(Z_0 [J]\) with respect to \(J\); so we can express the full functinal as 
%
\begin{align}
Z[J] &= \exp( i\int \dd{x} \mathscr{L}_I\qty( \frac{1}{i} \fdv{}{J(x)}))
\underbrace{\int \mathcal{D} \phi \exp(i \int \dd{x} \qty( \mathscr{L}_0 + J \phi )) }_{= Z_0 [J]}  \\
&= \sum _{n=0}^{\infty } \frac{i^{n}}{n!} \qty[ \int \dd{x} \mathscr{L}_I\qty(\frac{1}{i} \fdv{}{J(x)})]^{n} Z_0 [J]
\,.
\end{align}

We can use this to compute the Green's functions: 
%
\begin{align}
G(x_1 , \dots , x_n) = \eval{\frac{1}{i^{n}} \frac{ \delta Z[J]}{ \delta J (x_1 ) \dots \delta J(x_n)}}_{J = 0}
\,.
\end{align}

% \todo[inline]{Review from the PI notes why this makes sense.}
A common technique in QFT is the \textbf{Wick rotation} \(t \to -i t_E\), needed in order to move from a generally ill-defined oscillatory path integral of an exponential \(\exp(i S[q])\) in 4D space with an indefinite signature, to a Euclidean path integral of an exponential \(\exp(- S_E[q])\) in 4D space with a Cartesian signature, so that the D'Alambert operator is mapped to the Laplace-Beltrami operator.

\end{document}
