\documentclass[main.tex]{subfiles}
\begin{document}

\section{Path integral basics}

Following \cite[]{zaidiFunctionalMethods1983}.

We start from the space of square-integrable functions \(q(x)\), endowed with a product and an orthonormal basis \(\phi _n\).
We consider (multi-)linear \emph{functionals}, which are maps from the space of square-integrable functions (or from tuples of them) to \(\mathbb{R}\) or \(\mathbb{C}\). 
These can be represented as functions of infinitely many variables, countably so if we use the basis \(\phi _n\), uncountably so if we use the continuous basis \(x\).

A functional \(F[q]\) can be represented as a power series 
%
\begin{align}
F[q] = \sum _{n=0}^{\infty } \frac{1}{n!} \prod_{i=1}^{n} \int \dd{x_i} q(x_i) f(x_1, \dots, x_n)
\,.
\end{align}

Examples of this are the exponential series corresponding to the function \(f(x)\), mapping \(q(x)\) to \(e^{(f, q)}\) where the brackets denote the scalar product in the space, and the Gaussian series corresponding to the kernel \(K(x, y)\), mapping \(q(x)\) to \(e^{(q, K, q)}\), where 
%
\begin{align}
(q, K, q) = \int \dd{x} \dd{y} q(x) q(y) K(x, y)
\,.
\end{align}

\textbf{Functional derivatives} describes how the output of the functional changes as the argument goes from \(q(x)\) to \(q(x) + \eta (x)\), where \(\eta (x)\) is small. 
This will be a linear functional of \(\eta \) to first order, so we define the functional derivative with the expression 
%
\begin{align}
\eval{F[q+\eta ] - F[q]}_{\text{linear order}} = \int \eta (y) \fdv{F}{q (y)} \dd{y}
\,.
\end{align}

The analogy to finite-dimensional spaces is as follows: the functional derivative \(\fdv*{F}{q(y)}\) corresponds to the \emph{gradient} \(\nabla^{i} F\), while the integral in the previous expression corresponds to the \emph{directional derivative} \((\nabla^{i} F) \eta^{j} g_{ij}\).
The metric is present since the gradient is conventionally defined with a vector-like upper index; in our infinite-dimensional space the scalar product is given by the integral.

Practically speaking, the most convenient way to calculate a functional derivative is by taking \(\eta (x)\) to be such that it only differs from zero in a small region near \(y\), and let us define 
%
\begin{align}
\delta \omega = \int \eta (x) \dd{x}
\,.
\end{align}

Then, we define 
%
\begin{align}
\fdv{F}{q(y)} = \lim_{ \delta \omega \to 0} \frac{F[q + \eta ] - F[q]}{ \delta \omega }
\,.
\end{align}

In order for the limit to be computed easily, it is convenient for \(\eta (x)\) to be in the form \(\delta \omega \times \text{fixed function}\),
so that we are only changing the normalization as we shrink \(\delta \omega \).
A common choice is then 
%
\begin{align}
\eta (x) = \delta \omega  \delta (x-y)
\,.
\end{align}

If we apply this procedure to the identity functional \(q \to q\), we find 
%
\begin{align}
\fdv{q(x)}{q(y)} = \lim_{ \delta \omega  \to 0} \frac{q (x) + \delta \omega \delta (x-y) -q (x)}{ \delta \omega } = \delta (x-y)
\,.
\end{align}

The variable \(q\) is one-dimensional, if instead we wanted to consider a multi-dimensional coordinate system \(q_\alpha \) by the same reasoning we would find 
%
\begin{align}
\fdv{q_\alpha (x)}{q_\beta (y)} = \delta_{\alpha \beta } \delta (x-y)
\,.
\end{align}

An example: the functional derivative of a functional \(F_n\) defined by 
%
\begin{align}
F_n[q] = \int f(x_1 , \dots, x_n) q(x_1 )\dots q(x_n) \dd{x_1} \dots \dd{x_n}
\,,
\end{align}
%
where \(f\) is a symmetric function of its arguments, is given by 
%
\begin{align}
\fdv{F_n}{q(y)} = n \int f(x_1, \dots, x_{n-1}, y) q(x_1 )\dots q(x_{n-1}) \dd{x_1 } \dots \dd{x_{n-1}}
\,,
\end{align}
%
a function of \(y\). 

A \textbf{linear transformation} is in the form 
%
\begin{align}
q(x) = \int K(x, y) q'(y) \dd{y}
\,.
\end{align}

If this transformation has an inverse, which is characterized by the kernel \(K^{-1}\), then we must have the orthonormality relation 
%
\begin{align}
\int K(x,y) K^{-1} (y, z) \dd{y} =
\int K^{-1}(x,y) K (y, z) \dd{y} =
\delta (x-z)
\,.
\end{align}

We can do \textbf{Legendre transforms}: if we have a functional \(F\) we can differentiate with respect to the coordinate \(q\) to find 
%
\begin{align}
\fdv{F[q]}{q(x)} = p(x)
\,,
\end{align}
%
in analogy to the momentum in Lagrangian mechanics. Then, we can map \(F[q]\) to a new functional \(G[p]\) which will only depend on the momentum: 
%
\begin{align}
G[p]= F[p] - \int q(x) p(x) \dd{x}
\,.
\end{align}

We can also define functional integration, by 
%
\begin{align}
\int F[q] \qty[ \dd{q}] = \int \hat{F}(\qty{q_i}) \prod_i \dd{q_i}
\,.
\end{align}

On the right-hand side we are using the expression of the functional as a function of infinitely many variables which we discussed above;
we are then integrating over each of the coordinates in this infinite dimensional function space.
The infinite-dimensional measure is also often denoted as \(\mathcal{D}q\). 

This integral will not always exist, however in the cases in which it does we can change variables. 
Let us consider a linear change of variable, whose kernel is \(K(x, y)\), such that (compactly written) \(q = K q'\). 

Then, we want to compute the integral 
%
\begin{align}
\int F[Kq'] \qty[ \dd{Kq'}] 
\,
\end{align}
%
as an integral in \(\qty[ \dd{q}]\): in order to do so, we need to relate the two functional measures. 
We start by expressing both \(q\) and \(q'\) in terms of an orthonormal basis \(\phi _i\): inserting this into the linear transformation law we get 
%
\begin{align}
q(x) &= \int K(x, y) q'(y) \dd{y}  \\
\sum _{i} q_i \phi _i(x) &= \int K(x, y) \sum _{j} q'_j \phi _j (y) \dd{y}  \\
\sum _{i} q_i \underbrace{\int \phi _i (x) \phi _k (x) \dd{x}}_{ \delta_{ik}} 
&= 
\sum _{j} q'_j \underbrace{\int K(x, y) \phi _j (y) \phi _k (x) \dd{y} \dd{x}}_{ k_{jk}}  \\
q_k &= \sum _{j} q'_j k_{jk}
\,.
\end{align}

Then, the measure will transform with the determinant \(\det K = \det k\), which we can now express as an infinite product of the eigenvalues of \(k\): 
%
\begin{align}
\qty[ \dd{q}] = \abs{\pdv{q}{q'}} \qty[ \dd{q'}] = \det K \qty[ \dd{q'}]
\,.
\end{align}

Usually functional integrals cannot be computed analytically; the exception is given by Gaussian integrals, which generalize the finite-dimensional result 
%
\begin{align}
\int_{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + i b_j x_j) \dd{x_1 } \dots \dd{x_j} = \sqrt{\frac{(2 \pi)^{n}}{\det A}}
\exp(- \frac{1}{2} (A^{-1})_{ij} b_i b_j)
\,.
\end{align}

Here \(A_{ij}\) is an \(n\)-dimensional real matrix (which WLOG can be taken to be symmetric) while \(b_i\) is an \(n\)-dimensional vector.
The result comes from a transformation of the coordinates according to the finite-dimensional 

This can be interpreted as a ``functional'' (still finite-dimensional, so just a function, but we will generalize soon) of \(b_i\); we write it with an additional normalization \(N\) for convenience:
%
\begin{align}
Z[b] = N \int _{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + b_j x_j) \dd{x_1 } \dots \dd{x_j}
= N \sqrt{\frac{(2 \pi )^{n}}{\det A}} \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,,
\end{align}
%
and if we rescale the normalization \(N\) so that \(Z[\vec{0}] = 1 \) we get 
%
\begin{align}
Z[b] = \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,.
\end{align}

The infinite-dimensional generalization of this result amounts to replacing all the sums (expressed implicitly with Einstein notation here) with integrals; also conventionally we change the names of the variables to \(x \to q\), \(A \to K\), \(b \to J\): 
%
\begin{align}
Z[J] &= N \int \mathcal{D}q \exp(- \frac{1}{2} \int \dd{x} \dd{y} K(x, y) q(x) q(y) + i \int \dd{x} q(x) J(x))  \\
&= \exp(- \frac{1}{2} \int \dd{x} \dd{y} J(x) J(y) K^{-1}(x, y))
\,.
\end{align}

Let us now give some examples of applications of this result: \(K(x,y) = \sigma^{-2} \delta (x-y)\) means \(K^{-1}(x,y) = \sigma^2 \delta (x-y) \), so 
%
\begin{align}
Z[J] = \exp(- \frac{\sigma^2}{2} \int \dd{x} J^2(x))
\,.
\end{align}

This, as we shall see, can be used to give us a description of white noise, which is uncorrelated in momentum space.

Let us consider another example, whose physical application is to describe the motion of a massive scalar boson with Lagrangian 
%
\begin{align}
\mathscr{L} = \underbrace{\frac{1}{2} \partial_{\mu } \phi \partial^{\mu } \phi - \frac{1}{2} \mu^2 \phi^2}_{\mathscr{L}_{0}}+ \mathscr{L}_I (\phi )
\,,
\end{align}
%
where the self-interaction term is some non-quadratic function of \(\phi \), often taken to be proportional to  \(\phi^3\)  or \(\phi^{4}\). 

The Feynman path integral corresponding to this Lagrangian is given by the functional 
%
\begin{align}
Z[J] = N \int \mathcal{D} \phi \exp(i \int \mathscr{L}(\phi ) +  J \phi \dd{x} )
\,.
\end{align}

Let us start with the non-interacting case, that is, we compute \(Z_0 \) with only the quadratic term in the Lagrangian. This can be expressed, in the formalism from before, using the kernel 
%
\begin{align}
K(x, y )= (- \square_x - \mu^2 ) \delta (x-y)
\,.
\end{align}

Now, the expression the functional is given in terms of \(K^{-1}\): what is the inverse of this kernel? The definition reduces to 
%
\begin{align}
\int K(x, y) K^{-1}(y, z) \dd{y} &= \delta (x-z)   \\
- \qty(\square_x + \mu^2) K^{-1} (x, z) &= \delta (x-z)
\,,
\end{align}
%
which is readily solved in momentum space, with a \(+i \epsilon \) prescription in order to avoid the pole in the integration: what we find is called the \emph{Green's function}, 
%
\begin{align}
K^{-1}(x, z) = G(x-z) = \frac{1}{(2 \pi )^{4}} \int \frac{e^{-ik \cdot (x-z)}}{k^2 + \mu^2 - i \epsilon } \dd{k} 
\,,
\end{align}
%
so the unperturbed functional reads 
%
\begin{align}
Z_0 [J] = \exp(- \frac{i}{2} \int \dd{x} \dd{y} G(x-y) J(x) J(y))
\,.
\end{align}

% This, evaluated at \(J =0\), can be used to calculate the \emph{propagator}, which quantifies the probability amplitude that a particle localized at a certain position at a certain time will be found at another position, at another time.
% This can be used to compute expectation 

This by itself might not seem very useful, the motion of a free massive boson can be calculated with easier methods.
However, the real power of this path integral is the possibility to write the interacting term perturbatively: the interaction Lagrangian is a function of \(\phi \), which is what we find if we perform a functional integration of the argument of the exponential in \(Z_0 [J]\) with respect to \(J\); so we can express the full functinal as 
%
\begin{align}
Z[J] &= \exp( i\int \dd{x} \mathscr{L}_I\qty( \frac{1}{i} \fdv{}{J(x)}))
\underbrace{\int \mathcal{D} \phi \exp(i \int \dd{x} \qty( \mathscr{L}_0 + J \phi )) }_{= Z_0 [J]}  \\
&= \sum _{n=0}^{\infty } \frac{i^{n}}{n!} \qty[ \int \dd{x} \mathscr{L}_I\qty(\frac{1}{i} \fdv{}{J(x)})]^{n} Z_0 [J]
\,.
\end{align}

We can use this to compute the Green's functions: 
%
\begin{align}
G(x_1 , \dots , x_n) = \eval{\frac{1}{i^{n}} \frac{ \delta Z[J]}{ \delta J (x_1 ) \dots \delta J(x_n)}}_{J = 0}
\,.
\end{align}

\todo[inline]{Review from the PI notes why this makes sense.}

\subsection{The probability density functional}

We can interpret the quantity 
%
\begin{align}
\exp(- \frac{1}{2} (q, K, q)) \mathcal{D}q
\,
\end{align}
%
as a \emph{probability density functional} \(\dd{P}[q]\), since 
\begin{enumerate}
    \item it is positive definite;
    \item it is normalized, as long as we set its integral, \(Z[0]\), to 1;
    \item it goes to zero as \(q \to \pm \infty \).
\end{enumerate}

If this is the case, then we ought to be able to compute the average value of a functional \(F[q]\) as 
%
\begin{align}
\expval{F[q]} = \int F[q] \dd{P}[q] = \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) F[q]  
\,,
\end{align}
%
which we can generalize to any non-gaussian probability density functional by replacing the exponential \(\exp(- \frac{1}{2} (q, K, q))\) with a generic \(\mathcal{P}[q]\).

A useful kind of average we can compute is given by the \(N\)-point correlation function, 
%
\begin{align}
C^{(N)}(x_1, \dots, x_n) = \expval{q(x_1) \dots q(x_n)}
\,.
\end{align}

With the formula we gave earlier, this can be computed as 
%
\begin{align}
C^{(N)}(x_1, \dots , x_n)
= \int \mathcal{D}q \mathcal{P}[q] \prod_i q(x_i)
\,.
\end{align}

Here we can make use of a trick: going back to the Gaussian probability case, consider the functional derivative
%
\begin{align}
\eval{\frac{1}{i}\fdv{Z[J]}{J(x_1 )}}_{J = 0} &= \frac{1}{i} \eval{\fdv{}{J(x)}}_{J=0} \int \mathcal{D}q
\exp(- \frac{1}{2} (q, K, q) + i (J,q))  \\
&= \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) q(x_1 )
= \expval{q(x_1 )} = C^{(1)}(x_1 )
\,,
\end{align}
%
which actually holds for any probability density functional, we did not make use of the gaussianity.
So, in general we will be able to write 
%
\begin{align}
C^{(N)} (x_1 \dots x_n) = \frac{1}{i^{N}} \eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 }
\,.
\end{align}

The correlation functions, which as we discussed in an earlier section are crucial when discussing structure formation, can be ``simply'' calculated by functional differentiation as long as we have the generating functional \(Z[J]\).
This generating functional is very similar mathematically to a partition function in statistical mechanics, and it serves an analogous role: its derivatives allow us to characterize the dynamics of the system. 

Now, any functional \(\mathscr{F} [q]\) can be expressed through a functional Taylor series:
%
\begin{align}
\mathscr{F}[q] = \sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} q(x_1) \dots q(x_n)
\,,
\end{align}
%
so if we compute the average value \(\expval{\mathscr{F}[q]}\) we find 
%
\begin{align}
\expval{\mathscr{F}[q]} &= 
\sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} \underbrace{\expval{q(x_1) \dots q(x_n)}}_{= C^{(N)} (x_1 \dots x_n)}  \\
&=\sum _{n=0}^{\infty } \frac{(-i)^{n}}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} 
\eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 } \\
&= \mathscr{F}\qty[ -i \fdv{}{J}]\eval{ Z[J]}_{J=0}
\,.
\end{align}

\todo[inline]{Formally this makes sense, but what does it mean to calculate the field at a derivation operator? Should this just be interpreted as a shorthand for the Taylor expansion or is there more to it?}

An example: consider a Gaussian field whose partition function \(Z[J]\) is given by 
%
\begin{align}
Z[J] = \exp( - \frac{1}{2} (J, K^{-1}, J))
\,.
\end{align}

Then, as before we can calculate the correlation functions through functional derivatives: the first ones are 
%
\begin{align}
\expval{q(x)} &= \frac{1}{i} \eval{\fdv{Z[J]}{J(x)}}_{J=0}  \\
&= \eval{-i \int \dd{y} K^{-1}(x, y) J(y) \exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0} =0\\
\expval{q(y)q(x)} &= -\eval{\frac{ \delta^2 Z[J]}{ \delta  J(y) \delta  J (x)}}_{J=0} \\
&= -\eval{\qty(-K^{-1}(x, y) + \int \dd{z} \dd{w} K^{-1} (x, z)K^{-1} (y, w) J(w))\exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0}  \\
&= K^{-1} (x, y)
\,.
\end{align}

So, we have our result: \emph{for a Gaussian variable, the two-point correlation function is the inverse of the kernel}. 
A similar, albeit quite long, calculation allows us to compute the \(N\)-point correlation function for the same Gaussian variable: we expand the exponential in \(Z[J]\) in a power series, and when we differentiate it an even number of times we find  
%
\begin{align}
C^{2N}(x_1 \dots x_{2N}) &= \qty[K^{-1}(x_1, x_2 ) K^{-1} (x_3, x_4) \dots K^{-1} (x_{2N-1}, x_{2N})]_{\text{symmetrized}}
\,,
\end{align}
%
where ``symmetrized'' means that we must sum over all the permutations of the variables \(x_i\) in the argument of the inverse kernels; on the other hand, the odd correlation functions \(C^{2N+1}\) all vanish since they correspond to the integrals of odd functions over all space.

We can also apply this process in reverse: starting from the two-point correlation function we can reconstruct the kernel, and with it the probability density functional \(\dd{P}[q]\). 

In the Gaussian case, as long as we know the two-point function, which corresponds to the inverse kernel, we can reconstruct the \(N\)-point function. 
This can also be stated by saying that the ``irreducible'' \(N\)-point functions are all zero except for \(N=2\), since all the higher ones can be reduced to that one. 

We shall see that all the reduced \(N\)-point functions can be recovered starting from the \emph{generating functional} of connected correlation functions:
%
\begin{align}
\mathscr{W}[J] = \log Z[J]
\,,
\end{align}
%
through the expansion 
%
\begin{align}
\mathscr{W}[J] = \sum _{n=1}^{\infty } \frac{i^{n}}{n!} \int \dd{x_1} \dots \dd{x_n} C^{N}_{C} (x_1 \dots x_N) J(x_1) \dots J(x_n)
\,.
\end{align}

These connected correlation functions \(C^{N}_{C}\) are not (in principle) related to the \(C^{N}\) from before. 
\todo[inline]{What is the relation between them, though?}
We could also have defined \(\mathscr{W}[J] = i \log Z[J]\), this is a matter of convention. 

Now, we define the \emph{classical field}
%
\begin{align}
q _{\text{cl}}(x) = \fdv{\mathscr{W}[J]}{J(x)}
\,,
\end{align}
%
and the effective action \(\Gamma [q _{\text{cl}}]\) as the Legendre transform of \(\mathscr{W}[J]\): 
%
\begin{align}
\Gamma [q _{\text{cl}}] = \mathscr{W}[J] - \int \dd{x} q _{\text{cl}} (x) J(x) 
\,,
\end{align}
%
from which we can then recover \(J(x)\) as 
%
\begin{align}
J(x) = - \fdv{\Gamma [q _{\text{cl}}]}{q _{\text{cl}}(x)}
\,.
\end{align}

In the Gaussian case we have 
%
\begin{align}
\mathscr{W}[J] = \log Z[J] = - \frac{1}{2} (J, K^{-1}, J)  
\,,
\end{align}
%
which, by direct comparison with the Taylor expansion, means that 
%
\begin{align}
C^{2}_{C} (x_1 , x_2 ) = K^{-1} (x_1 , x_2 )
\,,
\end{align}
%
while \(C^{N} \equiv 0\) for any \(N \neq 2\). 
Also, our expression for \(q _{\text{cl}}\) yields 
%
\begin{align}
q _{\text{cl}}(x) = -\int \dd{y} K^{-1}(x, y) J(y) 
\,,
\end{align}
%
from which we can express \(J(x)\) by using the direct kernel \(K(x, y)\): 
%
\begin{align}
\int \dd{x} q _{\text{cl}} (x) K (x, w) = - \int \dd{y} \dd{w} K^{-1}(x, y) K(x, w) J(y) = - \int \dd{w} \delta (y- w) J(y) = - J(w)
\,.
\end{align}

With an analogous procedure we can show that
%
\begin{align}
(J, K^{-1}, J) = (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

The effective action then reads
%
\begin{align}
\Gamma [q _{\text{cl}}] &= \eval{- \frac{1}{2} (J, K^{-1}, J) + (J, K^{-1}, J)}_{J = J(q _{\text{cl}})} \\
&= \frac{1}{2} (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

Now we have the tools to consider actual probabilities: starting from our classical field \(q\), we want to compute the probability that it takes on a certain value \(q \in (\alpha , \alpha + \dd{\alpha })\) at a point \(x\): this is expressed with a probability density function in the form 
%
\begin{align}
\dv{P_q}{\alpha } =P_{q(x)} (\alpha; x)
\,.
\end{align}

We want to write this ``\(P(\alpha ) \dd{\alpha }\)'' in terms of the functional integral; in order to do so, we start from the Fourier transform 
%
\begin{align}
\int \dd{\beta } \exp(i \beta \varphi ) P_q (\beta ; x) = \expval{e^{i \beta \varphi }}_{\beta }
\,.
\end{align}



\end{document}
