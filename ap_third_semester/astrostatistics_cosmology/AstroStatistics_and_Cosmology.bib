
@article{CMBForegroundConcise,
  title = {{{CMB}} Foreground: {{A}} Concise Review | {{Progress}} of {{Theoretical}} and {{Experimental Physics}} | {{Oxford Academic}}},
  url = {https://academic.oup.com/ptep/article/2014/6/06B109/1560725},
  urldate = {2020-12-29},
  file = {/home/jacopo/zotero/storage/BEJMU8BB/CMB foreground A concise review  Progress of The.pdf;/home/jacopo/zotero/storage/G8EFLKG6/1560725.html}
}

@article{eriksenJointBayesianComponent2008,
  title = {Joint {{Bayesian Component Separation}} and {{CMB Power Spectrum Estimation}}},
  author = {Eriksen, H. K. and Jewell, J. B. and Dickinson, C. and Banday, A. J. and GÃ³rski, K. M. and Lawrence, C. R.},
  date = {2008-03-20},
  journaltitle = {The Astrophysical Journal},
  shortjournal = {ApJ},
  volume = {676},
  pages = {10--32},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/525277},
  url = {https://iopscience.iop.org/article/10.1086/525277},
  urldate = {2020-12-29},
  abstract = {We describe and implement an exact, flexible, and computationally efficient algorithm for joint component separation and CMB power spectrum estimation, building on a Gibbs sampling framework. Two essential new features are (1) conditional sampling of foreground spectral parameters and (2) joint sampling of all amplitude-type degrees of freedom (e.g., CMB, foreground pixel amplitudes, and global template amplitudes) given spectral parameters. Given a parametric model of the foreground signals, we estimate efficiently and accurately the exact joint foregroundCMB posterior distribution and, therefore, all marginal distributions such as the CMB power spectrum or foreground spectral index posteriors. The main limitation of the current implementation is the requirement of identical beam responses at all frequencies, which restricts the analysis to the lowest resolution of a given experiment. We outline a future generalization to multiresolution observations. To verify the method, we analyze simple models and compare the results to analytical predictions. We then analyze a realistic simulation with properties similar to the 3 yr WMAP data, downgraded to a common resolution of 3  FWHM. The results from the actual 3 yr WMAP temperature analysis are presented in a companion Letter.},
  file = {/home/jacopo/zotero/storage/UNB9GIPD/Eriksen et al. - 2008 - Joint Bayesian Component Separation and CMB Power .pdf},
  langid = {english},
  number = {1}
}

@article{eriksenPowerSpectrumEstimation2004,
  title = {Power Spectrum Estimation from High-Resolution Maps by {{Gibbs}} Sampling},
  author = {Eriksen, H. K. and O'Dwyer, I. J. and Jewell, J. B. and Wandelt, B. D. and Larson, D. L. and Gorski, K. M. and Levin, S. and Banday, A. J. and Lilje, P. B.},
  date = {2004-12},
  journaltitle = {The Astrophysical Journal Supplement Series},
  shortjournal = {ASTROPHYS J SUPPL S},
  volume = {155},
  pages = {227--241},
  issn = {0067-0049, 1538-4365},
  doi = {10.1086/425219},
  url = {http://arxiv.org/abs/astro-ph/0407028},
  urldate = {2021-01-02},
  abstract = {We revisit a recently introduced power spectrum estimation technique based on Gibbs sampling, with the goal of applying it to the high-resolution WMAP data. In order to facilitate this analysis, a number of sophistications have to be introduced, each of which is discussed in detail. We have implemented two independent versions of the algorithm to cross-check the computer codes, and to verify that a particular solution to any given problem does not affect the scientific results. We then apply these programs to simulated data with known properties at intermediate (N\_side = 128) and high (N\_side = 512) resolutions, to study effects such as incomplete sky coverage and white vs. correlated noise. From these simulations we also establish the Markov chain correlation length as a function of signal-to-noise ratio, and give a few comments on the properties of the correlation matrices involved. Parallelization issues are also discussed, with emphasis on real-world limitations imposed by current super-computer facilities. The scientific results from the analysis of the first-year WMAP data are presented in a companion letter.},
  archivePrefix = {arXiv},
  eprint = {astro-ph/0407028},
  eprinttype = {arxiv},
  file = {/home/jacopo/zotero/storage/45YF4SBN/Eriksen et al. - 2004 - Power spectrum estimation from high-resolution map.pdf;/home/jacopo/zotero/storage/ZNPG6RLQ/0407028.html},
  keywords = {Astrophysics},
  number = {2}
}

@article{heavensModelSelectionForecasting2007,
  title = {On Model Selection Forecasting, {{Dark Energy}} and Modified Gravity},
  author = {Heavens, A. F. and Kitching, T. D. and Verde, L.},
  date = {2007-08-20},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {380},
  pages = {1029--1035},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2007.12134.x},
  url = {http://arxiv.org/abs/astro-ph/0703191},
  urldate = {2020-12-22},
  abstract = {The Fisher matrix approach (Fisher 1935) allows one to calculate in advance how well a given experiment will be able to estimate model parameters, and has been an invaluable tool in experimental design. In the same spirit, we present here a method to predict how well a given experiment can distinguish between different models, regardless of their parameters. From a Bayesian viewpoint, this involves computation of the Bayesian evidence. In this paper, we generalise the Fisher matrix approach from the context of parameter fitting to that of model testing, and show how the expected evidence can be computed under the same simplifying assumption of a gaussian likelihood as the Fisher matrix approach for parameter estimation. With this `Laplace approximation' all that is needed to compute the expected evidence is the Fisher matrix itself. We illustrate the method with a study of how well upcoming and planned experiments should perform at distinguishing between Dark Energy models and modified gravity theories. In particular we consider the combination of 3D weak lensing, for which planned and proposed wide-field multi-band imaging surveys will provide suitable data, and probes of the expansion history of the Universe, such as proposed supernova and baryonic acoustic oscillations surveys. We find that proposed large-scale weak lensing surveys from space should be able readily to distinguish General Relativity from modified gravity models.},
  archivePrefix = {arXiv},
  eprint = {astro-ph/0703191},
  eprinttype = {arxiv},
  file = {/home/jacopo/zotero/storage/4I8ZNWRZ/Heavens et al. - 2007 - On model selection forecasting, Dark Energy and mo.pdf;/home/jacopo/zotero/storage/732CKEAP/0703191.html},
  keywords = {Astrophysics},
  number = {3}
}

@article{hobsonBayesianMethodsCosmology,
  ids = {hobsonBayesianMethodsCosmologya},
  title = {Bayesian {{Methods}} in {{Cosmology}}},
  author = {Hobson, Michael P and Jaffe, Andrew H and Liddle, Andrew R and Mukherjee, Pia and Parkinson, David},
  pages = {317},
  file = {/home/jacopo/zotero/storage/S4BNMNTG/Hobson et al. - Bayesian Methods in Cosmology.pdf;/home/jacopo/zotero/storage/X44YNYGH/Hobson et al. - Bayesian Methods in Cosmology.pdf},
  langid = {english}
}

@article{hobsonCombiningCosmologicalDatasets2002,
  title = {Combining Cosmological Datasets: Hyperparameters and {{Bayesian}} Evidence},
  shorttitle = {Combining Cosmological Datasets},
  author = {Hobson, M. P. and Bridle, S. L. and Lahav, O.},
  date = {2002-09},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  volume = {335},
  pages = {377--388},
  issn = {0035-8711, 1365-2966},
  doi = {10.1046/j.1365-8711.2002.05614.x},
  url = {http://arxiv.org/abs/astro-ph/0203259},
  urldate = {2020-12-21},
  abstract = {A method is presented for performing joint analyses of cosmological datasets, in which the weight assigned to each dataset is determined directly by it own statistical properties. The weights are considered in a Bayesian context as a set of hyperparameters, which are then marginalised over in order to recover the posterior distribution as a function only of the cosmological parameters of interest. In the case of a Gaussian likelihood function, this marginalisation may be performed analytically. Calculation of the Bayesian evidence for the data, with and without the introduction of hyperparameters, enables a direct determination of whether the data warrant the introduction of weights into the analysis; this generalises the standard likelihood ratio approach to model comparison. The method is illustrated by application to the classic toy problem of fitting a straight line to a set of data. A cosmological illustration of the technique is also presented, in which the latest measurements of the cosmic microwave background power spectrum are used to infer constraints on cosmological parameters.},
  archivePrefix = {arXiv},
  eprint = {astro-ph/0203259},
  eprinttype = {arxiv},
  file = {/home/jacopo/zotero/storage/Z9JHFDP2/Hobson et al. - 2002 - Combining cosmological datasets hyperparameters a.pdf;/home/jacopo/zotero/storage/F2G9SWHW/0203259.html},
  keywords = {Astrophysics},
  number = {2}
}

@article{kassBayesFactors1995,
  title = {Bayes {{Factors}}},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  date = {1995-06-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {90},
  pages = {773--795},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476572},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
  urldate = {2020-11-04},
  abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: â¢ From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. â¢ Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. â¢ Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. â¢ Bayes factors are very general and do not require alternative models to be nested. â¢ Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. â¢ In ânonstandardâ statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests. â¢ The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. â¢ When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. â¢ Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. â¢ Bayes factors are useful for guiding an evolutionary model-building process. â¢ It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476572},
  file = {/home/jacopo/zotero/storage/VSX44DSH/01621459.1995.html},
  keywords = {Bayesian hypothesis tests,BIC,Importance sampling,Laplace method,Markov chain Monte Carlo,Model selection,Monte Carlo integration,Posterior model probabilities,Posterior odds,Quadrature,Schwarz criterion,Sensitivity analysis,Strength of evidence},
  number = {430}
}

@article{lahavBayesianHyperparametersApproach2000,
  title = {Bayesian 'hyper-Parameters' Approach to Joint Estimation: The {{Hubble}} Constant from {{CMB}} Measurements},
  shorttitle = {Bayesian 'hyper-Parameters' Approach to Joint Estimation},
  author = {Lahav, O. and Bridle, S. L. and Hobson, M. P. and Lasenby, A. N. and Sodre, L.},
  date = {2000-07-11},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  shortjournal = {Monthly Notices of the Royal Astronomical Society},
  volume = {315},
  pages = {L45-L49},
  issn = {0035-8711, 1365-2966},
  doi = {10.1046/j.1365-8711.2000.03633.x},
  url = {https://academic.oup.com/mnras/article-lookup/doi/10.1046/j.1365-8711.2000.03633.x},
  urldate = {2020-12-15},
  abstract = {Recently several studies have jointly analysed data from different cosmological probes with the motivation of estimating cosmological parameters. Here we generalize this procedure to allow freedom in the relative weights of various probes. This is done by including in the joint x 2 function a set of `hyper-parameters', which are dealt with using Bayesian considerations. Â TvehreNyrjselisnmuÂplxtl2jienÂ:g(iwnalshgteeorareditNhomjf im,switnhhieimcnhizuaimnssgbueÂmr eoxsf2jud(nawitfahoeprmroeinpxtr2jsioipsresproednradtthaaetasleostegjt)o.j)fWwtheeeiphllryuopspetorras-tpeeatrotahmme eimnteiemrtshi,ozides by estimating the Hubble constant H0 from different sets of recent cosmic microwave background (CMB) experiments (including Saskatoon, Python V, MSAM1, TOCO and Boomerang). The approach can be generalized for combinations of cosmic probes, and for other priors on the hyper-parameters.},
  file = {/home/jacopo/zotero/storage/7DIN2U6M/Lahav et al. - 2000 - Bayesian 'hyper-parameters' approach to joint esti.pdf},
  langid = {english},
  number = {4}
}

@article{liguoriAstrostatisticsCosmologySlides,
  title = {Astrostatistics and {{Cosmology}} Slides},
  author = {Liguori, Michele},
  pages = {168},
  file = {/home/jacopo/zotero/storage/YKAPNLIZ/Liguori - Astrostatistics and Cosmology.pdf},
  langid = {english}
}

@online{marchImprovedConstraintsCosmological2011,
  title = {Improved Constraints on Cosmological Parameters from {{SNIa}} Data},
  author = {March, M. C. and Trotta, R. and Berkes, P. and Starkman, G. D. and Vaudrevange, P. M.},
  date = {2011-08-08},
  doi = {10.1111/j.1365-2966.2011.19584.x},
  url = {http://arxiv.org/abs/1102.3237},
  urldate = {2020-11-23},
  abstract = {We present a new method based on a Bayesian hierarchical model to extract constraints on cosmological parameters from SNIa data obtained with the SALT-II lightcurve fitter. We demonstrate with simulated data sets that our method delivers tighter statistical constraints on the cosmological parameters over 90\% of the time, that it reduces statistical bias typically by a factor \textasciitilde{} 2-3 and that it has better coverage properties than the usual chi-squared approach. As a further benefit, a full posterior probability distribution for the dispersion of the intrinsic magnitude of SNe is obtained. We apply this method to recent SNIa data, and by combining them with CMB and BAO data we obtain Omega\_m=0.28 +/- 0.02, Omega\_Lambda=0.73 +/- 0.01 (assuming w=-1) and Omega\_m=0.28 +/- 0.01, w=-0.90 +/- 0.05 (assuming flatness; statistical uncertainties only). We constrain the intrinsic dispersion of the B-band magnitude of the SNIa population, obtaining sigma\_mu\^int = 0.13 +/- 0.01 [mag]. Applications to systematic uncertainties will be discussed in a forthcoming paper.},
  archivePrefix = {arXiv},
  eprint = {1102.3237},
  eprinttype = {arxiv},
  file = {/home/jacopo/zotero/storage/4UEFT2EF/March et al. - 2011 - Improved constraints on cosmological parameters fr.pdf;/home/jacopo/zotero/storage/YKCDTMGC/1102.html},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  primaryClass = {astro-ph}
}

@book{siviaDataAnalysisBayesian2006,
  title = {Data {{Analysis}}: {{A Bayesian Tutorial}}},
  shorttitle = {Data {{Analysis}}},
  author = {Sivia, Devinderjit and Skilling, John},
  date = {2006-06},
  publisher = {{Oxford University Press}},
  abstract = {Statistics lectures have been a source of much bewilderment and frustration for generations of students. This book attempts to remedy the situation by expounding a logical and unified approach to the whole subject of data analysis.  This text is intended as a tutorial guide for senior undergraduates and research students in science and engineering. After explaining the basic principles of Bayesian probability theory, their use is illustrated with a variety of examples ranging from elementary parameter estimation to image processing. Other topics covered include reliability analysis, multivariate optimization, least-squares and maximum likelihood, error-propagation, hypothesis testing, maximum entropy and experimental design.  The Second Edition of this successful tutorial book contains a new chapter on extensions to the ubiquitous least-squares procedure, allowing for the straightforward handling of outliers and unknown correlated noise, and a cutting-edge contribution from John Skilling on a novel numerical technique for Bayesian computation called 'nested sampling'.},
  eprint = {lYMSDAAAQBAJ},
  eprinttype = {googlebooks},
  file = {/home/jacopo/zotero/storage/854GGI8H/Sivia and Skilling - 2006 - Data Analysis A Bayesian Tutorial.pdf},
  isbn = {978-0-19-856831-5},
  keywords = {Mathematics / Applied,Mathematics / Probability & Statistics / Bayesian Analysis,Mathematics / Probability & Statistics / General,Science / Physics / General},
  langid = {english},
  pagetotal = {259}
}

@online{sokalMonteCarloMethods1996,
  title = {Monte {{Carlo Methods}} in {{Statistical Mechanics}}: {{Foundations}} and {{New Algorithms Note}} to the {{Reader}}},
  shorttitle = {Monte {{Carlo Methods}} in {{Statistical Mechanics}}},
  author = {Sokal, A.},
  date = {1996},
  url = {/paper/Monte-Carlo-Methods-in-Statistical-Mechanics%3A-and-Sokal/0bfe9e3db30605fe2d4d26e1a288a5e2997e7225},
  urldate = {2020-12-29},
  abstract = {These notes are an updated version of lectures given at the Cours de Troisi eme Cycle de la Physique en Suisse Romande (Lausanne, Switzerland) in June 1989. We thank the Troisi eme Cycle de la Physique en Suisse Romande and Professor Michel Droz for kindly giving permission to reprint these notes. The following notes are based on my course \textbackslash Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms\&quot; given at the Cours de Troisi eme Cycle de la Physique en Suisse Romande (Lausanne, Switzerland) in June 1989, and on my course \textbackslash Multi-Grid Monte Carlo for Lattice Field Theories\&quot; given at the Winter College on Multilevel Techniques in Computational Physics (Trieste, Italy) in January\{February 1991. The reader is warned that some of this material is out-of-date (this is particularly true as regards reports of numerical work). For lack of time, I have made no attempt to update the text, but I have added footnotes marked \textbackslash Note Added 1996\&quot; that correct a few errors and give additional bibliography. My rst two lectures at Carg ese 1996 were based on the material included here. My third lecture described the new nite-size-scaling extrapolation\vphantom\}},
  file = {/home/jacopo/zotero/storage/KHASXCIR/0bfe9e3db30605fe2d4d26e1a288a5e2997e7225.html},
  langid = {english}
}

@online{vanderplasFrequentismBayesianismIII2014,
  title = {Frequentism and {{Bayesianism III}}: {{Confidence}}, {{Credibility}}, and Why {{Frequentism}} and {{Science}} Do Not {{Mix}} | {{Pythonic Perambulations}}},
  author = {Vanderplas, Jake},
  date = {2014-06-12},
  url = {http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/},
  urldate = {2020-10-26},
  file = {/home/jacopo/zotero/storage/KEXNPXWF/frequentism-and-bayesianism-3-confidence-credibility.html}
}

@article{verdeFirstYearWilkinson2003,
  ids = {spergelFirstYearWilkinson2003},
  title = {First {{Year Wilkinson Microwave Anisotropy Probe}} ({{WMAP}}) {{Observations}}: {{Parameter Estimation Methodology}}},
  shorttitle = {First {{Year Wilkinson Microwave Anisotropy Probe}} ({{WMAP}}) {{Observations}}},
  author = {Verde, L. and Peiris, H. V. and Spergel, D. N. and Nolta, M. and Bennett, C. L. and Halpern, M. and Hinshaw, G. and Jarosik, N. and Kogut, A. and Limon, M. and Meyer, S. S. and Page, L. and Tucker, G. S. and Wollack, E. and Wright, E. L.},
  date = {2003-09},
  journaltitle = {The Astrophysical Journal Supplement Series},
  shortjournal = {ASTROPHYS J SUPPL S},
  volume = {148},
  pages = {195--211},
  issn = {0067-0049, 1538-4365},
  doi = {10.1086/377335},
  url = {http://arxiv.org/abs/astro-ph/0302218},
  urldate = {2020-11-17},
  abstract = {We describe our methodology for comparing the WMAP measurements of the cosmic microwave background (CMB) and other complementary data sets to theoretical models. The unprecedented quality of the WMAP data, and the tight constraints on cosmological parameters that are derived, require a rigorous analysis so that the approximations made in the modeling do not lead to significant biases. We describe our use of the likelihood function to characterize the statistical properties of the microwave background sky. We outline the use of the Monte Carlo Markov Chains to explore the likelihood of the data given a model to determine the best fit cosmological parameters and their uncertainties. We add to the WMAP data the l{$>\sptilde$}700 CBI and ACBAR measurements of the CMB, the galaxy power spectrum at z\textasciitilde 0 obtained from the 2dF galaxy redshift survey (2dFGRS), and the matter power spectrum at z\textasciitilde 3 as measured with the Ly alpha forest. These last two data sets complement the CMB measurements by probing the matter power spectrum of the nearby universe. Combining CMB and 2dFGRS requires that we include in our analysis a model for galaxy bias, redshift distortions, and the non-linear growth of structure. We show how the statistical and systematic uncertainties in the model and the data are propagated through the full analysis.},
  archivePrefix = {arXiv},
  eprint = {astro-ph/0302218},
  eprinttype = {arxiv},
  file = {/home/jacopo/zotero/storage/GQUHTV6S/Spergel et al. - 2003 - First Year Wilkinson Microwave Anisotropy Probe (W.pdf;/home/jacopo/zotero/storage/MCTA8FKR/Verde et al. - 2003 - First Year Wilkinson Microwave Anisotropy Probe (W.pdf;/home/jacopo/zotero/storage/XGFJN5GP/0302209.html;/home/jacopo/zotero/storage/YG3ALRRW/0302218.html},
  keywords = {Astrophysics},
  number = {1}
}

@article{wandeltGlobalExactCosmic2003,
  title = {Global, {{Exact Cosmic Microwave Background Data Analysis Using Gibbs Sampling}}},
  author = {Wandelt, Benjamin D. and Larson, David L. and Lakshminarayanan, Arun},
  date = {2003-10-02},
  doi = {10.1103/PhysRevD.70.083511},
  url = {https://arxiv.org/abs/astro-ph/0310080v2},
  urldate = {2021-01-02},
  abstract = {We describe an efficient and exact method that enables global Bayesian analysis of cosmic microwave background (CMB) data. The method reveals the joint posterior density (or likelihood for flat priors) of the power spectrum \$C\_\textbackslash ell\$ and the CMB signal. Foregrounds and instrumental parameters can be simultaneously inferred from the data. The method allows the specification of a wide range of foreground priors. We explicitly show how to propagate the non-Gaussian dependency structure of the \$C\_\textbackslash ell\$ posterior through to the posterior density of the parameters. If desired, the analysis can be coupled to theoretical (cosmological) priors and can yield the posterior density of cosmological parameter estimates directly from the time-ordered data. The method does not hinge on special assumptions about the survey geometry or noise properties, etc. It is based on a Monte Carlo approach and hence parallelizes trivially. No trace or determinant evaluations are necessary. The feasibility of this approach rests on the ability to solve the systems of linear equations which arise. These are of the same size and computational complexity as the map-making equations. We describe a pre-conditioned conjugate gradient technique that solves this problem and demonstrate in a numerical example that the computational time required for each Monte Carlo sample scales as \$n\_p\^\{3/2\}\$ with the number of pixels \$n\_p\$. We test our method using the COBE-DMR data and explore the non-Gaussian joint posterior density of the COBE-DMR \$C\_\textbackslash ell\$ in several projections.},
  file = {/home/jacopo/zotero/storage/KH2QAW8D/Wandelt et al. - 2003 - Global, Exact Cosmic Microwave Background Data Ana.pdf;/home/jacopo/zotero/storage/TD5NP2TX/0310080.html},
  langid = {english}
}

@online{winsteinWhatDifferenceConfidence,
  title = {What's the Difference between a Confidence Interval and a Credible Interval?},
  author = {Winstein, Keith},
  url = {https://stats.stackexchange.com/q/2287},
  howpublished = {Cross Validated}
}


